{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64fe4451",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fralfaro/MAT281_2023/blob/main/docs/labs/lab_032.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# MAT281 - Laboratorio N°03\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a15921",
   "metadata": {},
   "source": [
    "Esta semana revisaremos datos del **Índice de Libertad de Prensa** que confecciona cada año la asociación de Reporteros Sin Fronteras.\n",
    "\n",
    "> **Nota**: el conjunto a utilizar lo encuentra en el siguiente [link](https://drive.google.com/drive/folders/1zxiYb5ji_xa5_5tWxvdjCEGYRY9YvjR7?usp=drive_link).\n",
    "\n",
    "## Diccionario de datos\n",
    "\n",
    "\n",
    "|Variable       |Clase               |Descripción |\n",
    "|:--------------|:-------------------|:-----------|\n",
    "| codigo_iso | caracter | Código ISO del país |\n",
    "| pais | caracter | País |\n",
    "| anio | entero | Año del resultado |\n",
    "| indice | entero | Puntaje Índice Libertad de Prensa (menor puntaje = mayor libertad de prensa) |\n",
    "| ranking | entero | Ranking Libertad de Prensa |\n",
    "\n",
    "\n",
    "## Fuente original y adaptación\n",
    "Los datos fueron extraídos de [The World Bank](https://tcdata360.worldbank.org/indicators/h3f86901f?country=BRA&indicator=32416&viz=line_chart&years=2001,2019). La fuente original es [Reporteros sin Fronteras](https://www.rsf-es.org/). \n",
    "\n",
    "Por otro lado, estos archivos han sido modificado intencionalmente para ocupar todo lo aprendido en clases. A continuación, una breve descripción de cada uno de los data frames:\n",
    "\n",
    "* **libertad_prensa_codigo.csv**: contiene la información codigo_iso/pais. Existe un código que tiene dos valores.\n",
    "* **libertad_prensa_anio.csv**: contiene la información pais/anio/indice/ranking. Los nombres de las columnas estan en mayúscula.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed417506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a58320",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] El sistema no puede encontrar la ruta especificada: '/content/data/data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9712\\3381893116.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/content/data/data/\"\u001b[0m \u001b[1;31m#Crear una carpeta data y adentro de ella otra llamada data y ahi meter los archivos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0marchivos_anio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'libertad_prensa_codigo'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_codigos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'libertad_prensa_codigo.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] El sistema no puede encontrar la ruta especificada: '/content/data/data/'"
     ]
    }
   ],
   "source": [
    "path = \"/content/data/data/\" #Crear una carpeta data y adentro de ella otra llamada data y ahi meter los archivos\n",
    "\n",
    "archivos_anio = [path + f for f in listdir(path) if 'libertad_prensa_codigo' not in f ] \n",
    "df_codigos = pd.read_csv(path + 'libertad_prensa_codigo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894056d",
   "metadata": {},
   "source": [
    " El objetivo es tratar de obtener la mayor información posible de este conjunto de datos. Para cumplir este objetivo debe resolver las siguientes problemáticas:\n",
    " \n",
    "1. Lo primero será juntar toda la información en un _solo archivo_, para ello necesitamos seguir los siguientes pasos:\n",
    "\n",
    " * a) Crear el archivo **df_anio**, que contenga la información de **libertad_prensa_anio.csv** para cada año. Luego, normalice el nombre de las columnas a minúscula.\n",
    " * b) Encuentre y elimine el dato que esta duplicado en el archivo **df_codigo**.\n",
    " * c) Crear el archivo **df** que junte la información del archivo **df_anio** con **df_codigo** por la columna _codigo_iso_.\n",
    " \n",
    "> **Hint**: Para juntar por _anio_ ocupe la función **pd.concat**. Para juntar información por columna ocupe **pd.merge**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfd268a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\nico6\\\\OneDrive\\\\Escritorio\\\\data\\\\datalibertad_prensa_2018.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8504\\3712310966.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlista\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#crea una lista vacía para llenarla con los dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marchivos_anio\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdf_temporal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#se lee cada archivo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mlista\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_temporal\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#se agregan a la lista\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf_anio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlista\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#se juntan todas las listas para hacer una lista de listas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\nico6\\\\OneDrive\\\\Escritorio\\\\data\\\\datalibertad_prensa_2018.csv'"
     ]
    }
   ],
   "source": [
    "# respuesta\n",
    "\n",
    "#1a)\n",
    "\n",
    "list = [] #crea una lista vacía para llenarla con los dataframes\n",
    "for name in archivos_anio:\n",
    "    df_temp = pd.read_csv(name) #se lee cada archivo\n",
    "    lista.append(df_temp) #se agregan a la lista\n",
    "df_anio = pd.concat(list) #Se juntan todas las listas para hacer una lista de listas\n",
    "df_anio.columns = df_anio.columns.str.lower() #Se cambia el nombre de las columnas a minusculas\n",
    "df_anio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6333e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1b)\n",
    "\n",
    "df_codigos['codigo_iso'].value_counts() #cuenta que hay un valor duplicado (localizado)\n",
    "df_codigos = df_codigos[df_codigos['pais']!='malo'] #lo elimina de la lista\n",
    "df_codigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1c)\n",
    "\n",
    "df = pd.merge(df_anio,df_codigos,on='codigo_iso') #se usa la funcion merge para juntar\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e8301",
   "metadata": {},
   "source": [
    "2. Encontrar:\n",
    "   * ¿Cuál es el número de observaciones en el conjunto de datos?   \n",
    "   * ¿Cuál es el número de columnas en el conjunto de datos?   \n",
    "   * Imprime el nombre de todas las columnas  \n",
    "   * ¿Cuál es el tipo de datos de cada columna? \n",
    "   * Describir el conjunto de datos (**hint**: .describe())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14369830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# respuesta\n",
    "\n",
    "#a) #Cantidad de observaciones:\n",
    "\n",
    "cant_fil = df.shape[0] #ve la cantidad de filas que tiene el df\n",
    "print(\"La cantidad de observaciones es\", cant_fil)\n",
    "print(\"\") \n",
    "\n",
    "#b) #Cantidad de columnas\n",
    "\n",
    "cant_col = df.shape[1] #ve la cantidad de columnas que tiene el df\n",
    "print(\"La cantidad de columnas es\", cant_col)\n",
    "print(\"\")\n",
    "\n",
    "#c) #Imprimir el nombre de todas las columnas\n",
    "\n",
    "print(\"El nombre de las columnas son:\")\n",
    "aux = df.columns.tolist() #creamos una lista con todos los elementos\n",
    "for j in range(len(aux)): #imprimimos cada uno por separado para no tenerlos en una lista\n",
    "  print(aux[j])\n",
    "print(\"\")\n",
    "\n",
    "#d)\n",
    "\n",
    "print(\"El tipo de dato de cada columna es:\")\n",
    "print(df.dtypes) #función que da el tipo de dato\n",
    "print(\"\")\n",
    "\n",
    "#e)\n",
    "print(\"El conjunto de datos descritos:\")\n",
    "print(df.describe(include='all')) #se tuiliza la función describe mencionada en el hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebfbbea",
   "metadata": {},
   "source": [
    "3. Desarrolle una función `resumen_df(df)` para encontrar el total de elementos distintos y vacíos por columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b571f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# respuesta\n",
    "\n",
    "def resumen_df(df):\n",
    "    \"\"\"\n",
    "    funcion resumen con elementos distintos y vacios\n",
    "    por columnas\n",
    "    \"\"\"\n",
    "    nombres = df.columns\n",
    "\n",
    "    unique_list = [] #se crea lista con elementos únicos\n",
    "    null_list = [] #se crea lista con elementos vacíos\n",
    "    \n",
    "    for j in nombres:\n",
    "      unico = (len(df[j].unique())) #se usa en la j-esima columna la funcion unique para ver los elementos que son nulos, y luego contarlos con len()\n",
    "      vacio = (df[j].isnull().sum()) #se hace el mismo método con la función isnull()\n",
    "      \n",
    "      if vacio!=0:      #la condición es necesaria, porque la función unique() cuenta los elementos nulos como un elemento único, pero nosotros \n",
    "         unico = unico-1; #no queremos contar este elemento, pues no aporta información útil, por lo que, si la cantidad delementos vacíos es\n",
    "                        #distinta de 0, se restará a la cantidad de elementos únicos.\n",
    "                        \n",
    "      unique_list.append(unico) #se agregan la cantidad de elementos distintos que hay por columna a la lista\n",
    "      null_list.append(vacio) #se agregan la cantidad de elementos vacíos que hay por columna a la lista\n",
    "  \n",
    "    result = pd.DataFrame({'nombres': nombres})\n",
    "    result['elementos_distintos'] = unique_list #ponemos cada componente de la lista unique en su correspondiente lugar en el df result\n",
    "    result['elementos_vacios'] = null_list #ponemos cada componente de la lista null en su correspondiente lugar en el df result\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ceaaaa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8504\\2526200391.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# retornar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresumen_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# retornar \n",
    "resumen_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98efcc39",
   "metadata": {},
   "source": [
    "4. Para los paises latinoamericano, encuentre por año  el país con mayor y menor `indice`.\n",
    "\n",
    " * a) Mediante un ciclo _for_.\n",
    " * b) Mediante un  _groupby_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9077a9bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3420119290.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\nico6\\AppData\\Local\\Temp\\ipykernel_8504\\3420119290.py\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    df_america = # FIX ME\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# respuesta\n",
    "\n",
    "america = ['ARG', 'ATG', 'BLZ', 'BOL', 'BRA', 'CAN', 'CHL', 'COL', 'CRI',\n",
    "       'CUB', 'DOM', 'ECU', 'GRD', 'GTM', 'GUY', 'HND', 'HTI', 'JAM',\n",
    "       'MEX', 'NIC', 'PAN', 'PER', 'PRY', 'SLV', 'SUR', 'TTO', 'URY',\n",
    "       'USA', 'VEN']\n",
    "\n",
    "df_america = df[df.codigo_iso.isin(america)] #Filtra el df inicial a través del codigo iso definido en america\n",
    "\n",
    "#Se hará un diccionario con los países de índice  menor y mayor para facilitar lectura\n",
    "\n",
    "#a)maximo\n",
    "\n",
    "anios=df['anio'].unique() #se crea una lista de los años\n",
    "\n",
    "dict_mayor = dict() #del ejemplo del profesor en clases, se crea un diccionario con los mayores elementos\n",
    "for k in anios:\n",
    "    df_aux = df_america.loc[lambda x: x['anio'] == k] # filtramos por cada año, definiendo un dataframe auxiliar\n",
    "    max_indice = df_aux['indice'].max()  # encontrar maximo de la columna objetivo con la función max()\n",
    "    df_aux = df_aux[df_aux['indice']==max_indice] #editamos df_aux para que solo queden los mayores valores del índice por año\n",
    "    paises = list(df_aux['pais']) #creamos lista de elementos de la columna que tendrán el nombre de los países\n",
    "    new_paises=[] #Se crea lista de nuevos países, así evitar los espacios nulos (se comrpueba con len()) y evitar los datos\n",
    "                  #que compartan indice (tomaremos solo el primero)\n",
    "    if len(paises)<1:\n",
    "      paises.append('No hay datos') \n",
    "      new_paises.append(paises[0])\n",
    "    elif len(paises)>=1:\n",
    "      new_paises.append(paises[0])\n",
    "    if (np.isnan(max_indice)==True): #si el valor max_indice llega a ser NaN se cambia por un 0\n",
    "      max_indice = 0\n",
    "    dict_mayor[k] = [new_paises[0],max_indice] #se crea tupla con el nombre del país y su índice en el diccionario\n",
    "\n",
    "sorted(dict_mayor.items()) #ordenar por año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77396fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a)minimo\n",
    "\n",
    "dict_menor = dict() #del ejemplo del profesor en clases, se crea un diccionario con los mayores elementos\n",
    "for k in anios:\n",
    "    df_aux = df_america.loc[lambda x: x['anio'] == k] # filtramos por cada año, definiendo un dataframe auxiliar\n",
    "    min_indice = df_aux['indice'].min()  # encontrar maximo de la columna objetivo con la función max()\n",
    "    df_aux = df_aux[df_aux['indice']==min_indice] #editamos df_aux para que solo queden los mayores valores del índice por año\n",
    "    paises = list(df_aux['pais']) #creamos lista de elementos de la columna que tendrán el nombre de los países\n",
    "    new_paises = [] #Se crea lista de nuevos países, así evitar los espacios nulos (se comrpueba con len()) y evitar los datos\n",
    "                  #que compartan indice (tomaremos solo el primero)\n",
    "    if len(paises)<1:\n",
    "      paises.append('No hay datos')\n",
    "      new_paises.append(paises[0])\n",
    "    elif len(paises)>=1:\n",
    "      new_paises.append(paises[0])\n",
    "    if (np.isnan(min_indice)==True): #si el valor min_indice llega a ser NaN se cambia por un 0\n",
    "      min_indice = 0\n",
    "    dict_menor[k] = [new_paises[0],min_indice]#se crea otro diccionario con el nombre del país y su índice en el diccionario\n",
    "\n",
    "sorted(dict_menor.items()) #ordenar por año\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) maximo\n",
    "max_indice = df_america.groupby('anio').indice.max() #Se asigna a cada año el mayor indice por país (agrupado por anio)\n",
    "data_max = df_america.merge(max_indice, on='anio', suffixes=('','_max')) #se crea un dataframe ordenado por año (de stackoverflow)\n",
    "data_max = data_max[data_max.indice==data_max.indice_max].drop(['indice_max','codigo_iso','ranking',], axis=1) #condición aplicada (de stackoverlow) y sacar lsa columnas innecesrias\n",
    "by_anio = data_max.sort_values('anio') #se ordena por anio\n",
    "by_anio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abafd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b)minimo\n",
    "min_indice = df_america.groupby('anio').indice.min() #Se asigna a cada año el menor indice por país (agrupado por anio)\n",
    "data_min = df_america.merge(min_indice, on='anio', suffixes=('','_min')) #se crea un dataframe ordenado por año (de stackoverflow)\n",
    "data_min = data_min[data_min.indice==data_min.indice_min].drop(['indice_min','codigo_iso','ranking',], axis=1) #condición aplicada (de stackoverlow) y sacar lsa columnas innecesrias\n",
    "by_anio = data_min.sort_values('anio') #se ordena por anio\n",
    "by_anio #se ordena por anio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91532f43",
   "metadata": {},
   "source": [
    "5. Para cada _país_, muestre el _indice_ máximo que alcanzo por _anio_. Para los datos nulos, rellene con el valor **0**.\n",
    "\n",
    "**Ejemplo**:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ob0qch1dsOjDOUuZXnCY0HU_3XPp19gV\" width = \"700\" align=\"center\"/>\n",
    "\n",
    "> **Hint**: Utilice la función **pd.pivot_table**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# respuesta\n",
    "table = pd.pivot_table(df, values='indice', index=['codigo_iso'], columns=['anio'], aggfunc=np.sum, fill_value=0) #Se usa la función del ejemplo\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
